{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5 SÄ±nÄ±flÄ± Afet HasarÄ± GÃ¶rÃ¼ntÃ¼ SÄ±nÄ±flandÄ±rma - Conv2d TabanlÄ± CNN\n",
        "\n",
        "Bu notebook, Kaggle'dan afet hasarÄ± gÃ¶rÃ¼ntÃ¼ veri setini kullanarak PyTorch ile Conv2d tabanlÄ± CNN modeli eÄŸitmeyi amaÃ§lamaktadÄ±r.\n",
        "\n",
        "## Proje Ã–zeti\n",
        "\n",
        "- **Veri Seti**: Kaggle - Disaster Damage 5-Class Dataset (kagglehub ile indirilecek)\n",
        "- **Model**: PyTorch CNN (Conv2d tabanlÄ±)\n",
        "- **SÄ±nÄ±flar**: 5 sÄ±nÄ±f afet hasarÄ± kategorisi\n",
        "- **GÃ¶rÃ¼ntÃ¼ Boyutu**: 224x224 piksel\n",
        "\n",
        "## Model Ã–zellikleri\n",
        "\n",
        "- **Mimari**: 3 Conv BloÄŸu (Conv2d, BatchNorm, ReLU, MaxPooling) + FC (Dropout 0.6)\n",
        "- **Optimizer**: Adam (lr=1e-4, weight_decay=1e-4)\n",
        "- **Loss**: CrossEntropyLoss\n",
        "- **Scheduler**: ReduceLROnPlateau (patience=7)\n",
        "- **Regularization**: Dropout (0.6), Early Stopping, Weight Decay\n",
        "- **Data Augmentation**: GÃ¼Ã§lendirilmiÅŸ (Rotation, Zoom, Shift, Horizontal Flip, ColorJitter - sadece eÄŸitim iÃ§in)\n",
        "- **Hedefler**: YÃ¼ksek Accuracy, DÃ¼ÅŸÃ¼k Loss\n",
        "\n",
        "## Ä°Ã§indekiler\n",
        "\n",
        "1. KÃ¼tÃ¼phaneleri YÃ¼kleme\n",
        "2. Veri Setini Ä°ndirme (Kagglehub)\n",
        "3. Veri Setini HazÄ±rlama\n",
        "   - 3.1 Dataset SÄ±nÄ±fÄ±\n",
        "   - 3.2 Data Augmentation ve Transformlar\n",
        "   - 3.3 Dataset ve DataLoader OluÅŸturma\n",
        "4. CNN Model TanÄ±mÄ±\n",
        "5. YardÄ±mcÄ± Fonksiyonlar\n",
        "6. EÄŸitim ve DoÄŸrulama FonksiyonlarÄ±\n",
        "7. Model EÄŸitimi\n",
        "8. SonuÃ§larÄ± GÃ¶rselleÅŸtirme\n",
        "9. Model Testi\n",
        "10. Ã–rnek Tahminler\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. KÃ¼tÃ¼phaneleri YÃ¼kleme\n",
        "\n",
        "Gerekli kÃ¼tÃ¼phaneler import ediliyor:\n",
        "- **PyTorch**: Model mimarisi ve eÄŸitim iÃ§in\n",
        "- **PIL**: GÃ¶rÃ¼ntÃ¼ iÅŸleme (truncated image desteÄŸi aktif)\n",
        "- **NumPy/Pandas**: Veri manipÃ¼lasyonu\n",
        "- **Matplotlib/Seaborn**: GÃ¶rselleÅŸtirme\n",
        "- **Scikit-learn**: Metrik hesaplama ve veri split\n",
        "- **Kagglehub**: Veri seti indirme\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Temel kÃ¼tÃ¼phaneler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageFile\n",
        "\n",
        "# Truncated image'larÄ± yÃ¼klemeye izin ver\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# Veri iÅŸleme\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "# GÃ¶rselleÅŸtirme\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Metrikler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Progress bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Kaggle veri indirme\n",
        "import kagglehub\n",
        "\n",
        "# Cihaz ayarlarÄ±\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"KullanÄ±lan cihaz: {device}\")\n",
        "\n",
        "print(\"âœ… TÃ¼m kÃ¼tÃ¼phaneler yÃ¼klendi!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Veri Setini Ä°ndirme (Kagglehub)\n",
        "\n",
        "Kaggle'dan `sarthaktandulje/disaster-damage-5class` veri seti indiriliyor.\n",
        "\n",
        "**Ä°ÅŸlemler:**\n",
        "- Kagglehub ile otomatik indirme\n",
        "- Dataset yapÄ±sÄ± otomatik tespit (Training/Testing veya direkt sÄ±nÄ±f klasÃ¶rleri)\n",
        "- SÄ±nÄ±f isimleri otomatik belirleniyor\n",
        "- Train ve test klasÃ¶r yollarÄ± belirleniyor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"â³ Veri seti indiriliyor (kagglehub ile)...\")\n",
        "\n",
        "# Kagglehub ile veri setini indir\n",
        "cache_path = kagglehub.dataset_download(\"sarthaktandulje/disaster-damage-5class\")\n",
        "\n",
        "print(f\"âœ… Ä°ndirme tamamlandÄ±. Konum: {cache_path}\")\n",
        "print(f\"ğŸ“‚ Path to dataset files: {cache_path}\")\n",
        "\n",
        "# Dataset yapÄ±sÄ±nÄ± bul - farklÄ± olasÄ± yapÄ±larÄ± kontrol et\n",
        "dataset_base = None\n",
        "possible_paths = [\n",
        "    os.path.join(cache_path, \"dataset\"),\n",
        "    os.path.join(cache_path, \"disaster-damage-5class\"),\n",
        "    os.path.join(cache_path, \"disaster_dataset\", \"disaster_dataset\"),  # Ä°Ã§ iÃ§e klasÃ¶r yapÄ±sÄ±\n",
        "    os.path.join(cache_path, \"disaster_dataset\"),\n",
        "    cache_path\n",
        "]\n",
        "\n",
        "# Ã–nce Training/Testing klasÃ¶rlerini ara\n",
        "for p in possible_paths:\n",
        "    train_dir = os.path.join(p, \"Training\")\n",
        "    if not os.path.exists(train_dir):\n",
        "        train_dir = os.path.join(p, \"train\")\n",
        "    if not os.path.exists(train_dir):\n",
        "        train_dir = os.path.join(p, \"Train\")\n",
        "    if os.path.exists(train_dir):\n",
        "        dataset_base = p\n",
        "        TRAIN_DIR = train_dir\n",
        "        # Test klasÃ¶rÃ¼nÃ¼ bul\n",
        "        test_dir = os.path.join(p, \"Testing\")\n",
        "        if not os.path.exists(test_dir):\n",
        "            test_dir = os.path.join(p, \"test\")\n",
        "        if not os.path.exists(test_dir):\n",
        "            test_dir = os.path.join(p, \"Test\")\n",
        "        TEST_DIR = test_dir if os.path.exists(test_dir) else None\n",
        "        break\n",
        "\n",
        "# EÄŸer Training/Testing yoksa, direkt sÄ±nÄ±f klasÃ¶rlerini ara\n",
        "if dataset_base is None:\n",
        "    for p in possible_paths:\n",
        "        # SÄ±nÄ±f klasÃ¶rlerini kontrol et (fire, flood, landslide, normal, smoke)\n",
        "        if os.path.exists(p):\n",
        "            subdirs = [d for d in os.listdir(p) \n",
        "                      if os.path.isdir(os.path.join(p, d)) and not d.startswith('.')]\n",
        "            # EÄŸer sÄ±nÄ±f isimleri varsa (fire, flood, landslide, normal, smoke gibi)\n",
        "            if len(subdirs) >= 3:  # En az 3 sÄ±nÄ±f olmalÄ±\n",
        "                # Ä°Ã§inde gÃ¶rÃ¼ntÃ¼ dosyasÄ± olan klasÃ¶rleri kontrol et\n",
        "                has_images = False\n",
        "                for subdir in subdirs:\n",
        "                    subdir_path = os.path.join(p, subdir)\n",
        "                    files = [f for f in os.listdir(subdir_path) \n",
        "                            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
        "                    if len(files) > 0:\n",
        "                        has_images = True\n",
        "                        break\n",
        "                if has_images:\n",
        "                    dataset_base = p\n",
        "                    TRAIN_DIR = p  # TÃ¼m veri burada, train/val/test olarak ayÄ±racaÄŸÄ±z\n",
        "                    TEST_DIR = None\n",
        "                    break\n",
        "\n",
        "# EÄŸer hala bulunamadÄ±ysa, cache_path'i kullan\n",
        "if dataset_base is None:\n",
        "    dataset_base = cache_path\n",
        "    TRAIN_DIR = cache_path\n",
        "    TEST_DIR = None\n",
        "\n",
        "print(f\"âœ… Dataset yolu bulundu: {dataset_base}\")\n",
        "print(f\"ğŸ“‚ Veri klasÃ¶rÃ¼: {TRAIN_DIR}\")\n",
        "if TEST_DIR:\n",
        "    print(f\"ğŸ“‚ Test klasÃ¶rÃ¼: {TEST_DIR} (AyrÄ± test seti mevcut)\")\n",
        "else:\n",
        "    print(f\"ğŸ“‚ Test klasÃ¶rÃ¼: Yok (TÃ¼m veri train/val/test olarak ayrÄ±lacak)\")\n",
        "\n",
        "# SÄ±nÄ±f isimlerini otomatik bul\n",
        "CLASS_NAMES = []\n",
        "if os.path.exists(TRAIN_DIR):\n",
        "    # Ã–nce direkt klasÃ¶rde sÄ±nÄ±flarÄ± ara\n",
        "    dirs = [d for d in os.listdir(TRAIN_DIR) \n",
        "            if os.path.isdir(os.path.join(TRAIN_DIR, d)) and not d.startswith('.')]\n",
        "    \n",
        "    # EÄŸer iÃ§inde gÃ¶rÃ¼ntÃ¼ dosyasÄ± varsa sÄ±nÄ±f olarak kabul et\n",
        "    for d in dirs:\n",
        "        dir_path = os.path.join(TRAIN_DIR, d)\n",
        "        files = [f for f in os.listdir(dir_path) \n",
        "                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
        "        if len(files) > 0:\n",
        "            CLASS_NAMES.append(d)\n",
        "    \n",
        "    CLASS_NAMES.sort()\n",
        "    NUM_CLASSES = len(CLASS_NAMES)\n",
        "    \n",
        "    if NUM_CLASSES > 0:\n",
        "        print(f\"âœ… SÄ±nÄ±flar otomatik tespit edildi: {CLASS_NAMES}\")\n",
        "        print(f\"ğŸ“Š Toplam sÄ±nÄ±f sayÄ±sÄ±: {NUM_CLASSES}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ SÄ±nÄ±f klasÃ¶rleri bulunamadÄ±, manuel belirlenmeli\")\n",
        "        # VarsayÄ±lan sÄ±nÄ±flar\n",
        "        CLASS_NAMES = ['fire', 'flood', 'landslide', 'normal', 'smoke']\n",
        "        NUM_CLASSES = 5\n",
        "        print(f\"âš ï¸ VarsayÄ±lan sÄ±nÄ±flar kullanÄ±lÄ±yor: {CLASS_NAMES}\")\n",
        "else:\n",
        "    print(\"âš ï¸ Dataset klasÃ¶rÃ¼ bulunamadÄ±!\")\n",
        "    CLASS_NAMES = ['fire', 'flood', 'landslide', 'normal', 'smoke']\n",
        "    NUM_CLASSES = 5\n",
        "    print(f\"âš ï¸ VarsayÄ±lan sÄ±nÄ±flar kullanÄ±lÄ±yor: {CLASS_NAMES}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Veri Setini HazÄ±rlama\n",
        "\n",
        "Veri setini PyTorch ile kullanÄ±labilir hale getirmek iÃ§in hazÄ±rlÄ±k yapÄ±lÄ±yor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Dataset SÄ±nÄ±fÄ±\n",
        "\n",
        "PyTorch `Dataset` sÄ±nÄ±fÄ±ndan tÃ¼retilen Ã¶zel `DisasterDataset` sÄ±nÄ±fÄ± tanÄ±mlanÄ±yor.\n",
        "\n",
        "**Ã–zellikler:**\n",
        "- GÃ¶rÃ¼ntÃ¼leri yÃ¼kler ve RGB formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r\n",
        "- Truncated (bozuk) gÃ¶rÃ¼ntÃ¼ hatalarÄ±nÄ± yakalar ve siyah gÃ¶rÃ¼ntÃ¼ dÃ¶ndÃ¼rÃ¼r\n",
        "- Transform iÅŸlemlerini uygular\n",
        "- Etiketleri dÃ¶ndÃ¼rÃ¼r\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DisasterDataset(Dataset):\n",
        "    \"\"\"Afet hasarÄ± gÃ¶rÃ¼ntÃ¼ veri seti sÄ±nÄ±fÄ±\"\"\"\n",
        "    \n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # GÃ¶rÃ¼ntÃ¼yÃ¼ yÃ¼kle - truncated image hatalarÄ±nÄ± handle et\n",
        "        try:\n",
        "            # ImageFile.LOAD_TRUNCATED_IMAGES zaten import edildi ve True yapÄ±ldÄ±\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            # GÃ¶rÃ¼ntÃ¼yÃ¼ yÃ¼kle ve boyutunu kontrol et\n",
        "            if image.size[0] == 0 or image.size[1] == 0:\n",
        "                raise ValueError(\"GÃ¶rÃ¼ntÃ¼ boyutu geÃ§ersiz\")\n",
        "        except Exception as e:\n",
        "            # Hata durumunda siyah gÃ¶rÃ¼ntÃ¼ dÃ¶ndÃ¼r (sessizce, sadece ilk birkaÃ§ hatayÄ± gÃ¶ster)\n",
        "            if not hasattr(self, '_error_count'):\n",
        "                self._error_count = 0\n",
        "            if self._error_count < 5:  # Ä°lk 5 hatayÄ± gÃ¶ster\n",
        "                print(f\"âš ï¸ Hata: {os.path.basename(img_path)} yÃ¼klenemedi: {str(e)[:60]}\")\n",
        "                self._error_count += 1\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "        \n",
        "        # DÃ¶nÃ¼ÅŸÃ¼mleri uygula\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n",
        "\n",
        "print(\"âœ… Dataset sÄ±nÄ±fÄ± tanÄ±mlandÄ±! (Truncated image desteÄŸi eklendi)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Data Augmentation ve Transformlar\n",
        "\n",
        "Hiperparametreler ve transformlar tanÄ±mlanÄ±yor.\n",
        "\n",
        "**Hiperparametreler:**\n",
        "- `IMG_SIZE`: 224x224 piksel\n",
        "- `BATCH_SIZE`: 32 (baÅŸlangÄ±Ã§, dataset boyutuna gÃ¶re otomatik ayarlanacak)\n",
        "- `EPOCHS`: 50 (maksimum, dataset boyutuna gÃ¶re otomatik ayarlanacak)\n",
        "- `LEARNING_RATE`: 1e-4\n",
        "\n",
        "**Training Transform:**\n",
        "- Resize (274x274) â†’ RandomCrop (224x224)\n",
        "- RandomRotation (Â±35Â°)\n",
        "- RandomAffine (kaydÄ±rma ve zoom)\n",
        "- RandomHorizontalFlip\n",
        "- ColorJitter (parlaklÄ±k, kontrast, doygunluk, renk tonu)\n",
        "- Normalize (ImageNet mean/std)\n",
        "\n",
        "**Validation Transform:**\n",
        "- Resize (224x224)\n",
        "- Normalize (ImageNet mean/std)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hiperparametreler (BaÅŸlangÄ±Ã§ deÄŸerleri - dataset yÃ¼klendikten sonra otomatik ayarlanacak)\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32  # BaÅŸlangÄ±Ã§ deÄŸeri, dataset boyutuna gÃ¶re ayarlanacak\n",
        "EPOCHS = 50  # Maksimum 50 epoch (dataset boyutuna gÃ¶re ayarlanacak)\n",
        "LEARNING_RATE = 1e-4  # AÅŸÄ±rÄ± dÃ¼ÅŸÃ¼k deÄŸil, Ã§ok yavaÅŸ warmup yok\n",
        "\n",
        "# Training iÃ§in gÃ¼Ã§lendirilmiÅŸ data augmentation (yÃ¼ksek accuracy, dÃ¼ÅŸÃ¼k loss iÃ§in)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE + 50, IMG_SIZE + 50)),\n",
        "    transforms.RandomCrop(IMG_SIZE),\n",
        "    transforms.RandomRotation(degrees=35),  # Rastgele dÃ¶ndÃ¼rme\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.85, 1.15)),  # KaydÄ±rma ve yakÄ±nlaÅŸtÄ±rma\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Yatay Ã§evirme\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Renk deÄŸiÅŸiklikleri\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation iÃ§in sadece normalize\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"âœ… Data Augmentation (GÃ¼Ã§lendirilmiÅŸ):\")\n",
        "print(\"  Training: Resize, RandomCrop, RandomRotation(35Â°), RandomAffine(shift+zoom), RandomHorizontalFlip, ColorJitter\")\n",
        "print(\"  Validation: Resize, Normalize\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Dataset ve DataLoader OluÅŸturma\n",
        "\n",
        "Veri seti train/val/test olarak bÃ¶lÃ¼nÃ¼yor ve PyTorch DataLoader'lar oluÅŸturuluyor.\n",
        "\n",
        "**Split Stratejisi:**\n",
        "- AyrÄ± test klasÃ¶rÃ¼ varsa: Train/Val (80/20) + Test (ayrÄ±)\n",
        "- Test klasÃ¶rÃ¼ yoksa: Train/Val/Test (70/15/15)\n",
        "\n",
        "**Otomatik Hiperparametre Ayarlama:**\n",
        "- **Batch Size**: Dataset boyutuna gÃ¶re (16/32/64)\n",
        "- **Epochs**: Dataset boyutuna gÃ¶re (30/50 maksimum)\n",
        "\n",
        "**Ã‡Ä±ktÄ±lar:**\n",
        "- Train/Val/Test dataset ve DataLoader nesneleri\n",
        "- SÄ±nÄ±f baÅŸÄ±na gÃ¶rÃ¼ntÃ¼ sayÄ±larÄ±\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TÃ¼m gÃ¶rÃ¼ntÃ¼ yollarÄ±nÄ± ve etiketlerini topla\n",
        "all_images = []\n",
        "all_labels = []\n",
        "for class_name in CLASS_NAMES:\n",
        "    class_dir = os.path.join(TRAIN_DIR, class_name)\n",
        "    if os.path.exists(class_dir):\n",
        "        class_idx = CLASS_NAMES.index(class_name)\n",
        "        for img_name in os.listdir(class_dir):\n",
        "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
        "                all_images.append(os.path.join(class_dir, img_name))\n",
        "                all_labels.append(class_idx)\n",
        "\n",
        "print(f\"ğŸ“Š Toplam gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±: {len(all_images)}\")\n",
        "\n",
        "# EÄŸer TEST_DIR varsa, test setini ayrÄ± topla\n",
        "# Yoksa tÃ¼m veriyi train/val/test olarak ayÄ±r\n",
        "if TEST_DIR and os.path.exists(TEST_DIR):\n",
        "    # Test seti ayrÄ± klasÃ¶rde\n",
        "    test_images = []\n",
        "    test_labels = []\n",
        "    for class_name in CLASS_NAMES:\n",
        "        class_dir = os.path.join(TEST_DIR, class_name)\n",
        "        if os.path.exists(class_dir):\n",
        "            class_idx = CLASS_NAMES.index(class_name)\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
        "                    test_images.append(os.path.join(class_dir, img_name))\n",
        "                    test_labels.append(class_idx)\n",
        "    \n",
        "    # Kalan veriyi train/val olarak ayÄ±r (80/20)\n",
        "    train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "        all_images, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n",
        "    )\n",
        "    print(f\"ğŸ“Š Test seti ayrÄ± klasÃ¶rden yÃ¼klendi: {len(test_images)} gÃ¶rÃ¼ntÃ¼\")\n",
        "else:\n",
        "    # TÃ¼m veriyi train/val/test olarak ayÄ±r (70/15/15)\n",
        "    # Ã–nce train ve geÃ§ici (val+test) olarak ayÄ±r (70/30)\n",
        "    train_images, temp_images, train_labels, temp_labels = train_test_split(\n",
        "        all_images, all_labels, test_size=0.3, random_state=42, stratify=all_labels\n",
        "    )\n",
        "    # Sonra val/test olarak ayÄ±r (50/50) - bu 30'un yarÄ±sÄ± = 15% val, 15% test\n",
        "    val_images, test_images, val_labels, test_labels = train_test_split(\n",
        "        temp_images, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
        "    )\n",
        "    print(f\"ğŸ“Š Veri train/val/test olarak ayrÄ±ldÄ± (70/15/15)\")\n",
        "\n",
        "# Dataset boyutuna gÃ¶re hiperparametreleri otomatik ayarla\n",
        "train_size = len(train_images)\n",
        "val_size = len(val_images)\n",
        "test_size = len(test_images)\n",
        "\n",
        "# Batch size ayarlama (dataset boyutuna gÃ¶re)\n",
        "if train_size < 500:\n",
        "    BATCH_SIZE = 16\n",
        "    print(\"âš ï¸  KÃ¼Ã§Ã¼k dataset tespit edildi, Batch Size: 16 olarak ayarlandÄ±\")\n",
        "elif train_size < 2000:\n",
        "    BATCH_SIZE = 32\n",
        "    print(\"âœ… Orta dataset tespit edildi, Batch Size: 32 olarak ayarlandÄ±\")\n",
        "else:\n",
        "    BATCH_SIZE = 64\n",
        "    print(\"âœ… BÃ¼yÃ¼k dataset tespit edildi, Batch Size: 64 olarak ayarlandÄ±\")\n",
        "\n",
        "# Epoch sayÄ±sÄ±nÄ± dataset boyutuna gÃ¶re ayarla (maksimum 50)\n",
        "if train_size < 500:\n",
        "    EPOCHS = 30\n",
        "    print(\"âš ï¸  KÃ¼Ã§Ã¼k dataset iÃ§in Epoch: 30 olarak ayarlandÄ±\")\n",
        "elif train_size < 2000:\n",
        "    EPOCHS = 50\n",
        "    print(\"âœ… Orta dataset iÃ§in Epoch: 50 olarak ayarlandÄ±\")\n",
        "else:\n",
        "    EPOCHS = 50  # Maksimum 50 epoch\n",
        "    print(\"âœ… BÃ¼yÃ¼k dataset iÃ§in Epoch: 50 olarak ayarlandÄ± (maksimum)\")\n",
        "\n",
        "# Dataset ve DataLoader oluÅŸtur\n",
        "train_dataset = DisasterDataset(train_images, train_labels, transform=train_transform)\n",
        "val_dataset = DisasterDataset(val_images, val_labels, transform=val_transform)\n",
        "test_dataset = DisasterDataset(test_images, test_labels, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "# SÄ±nÄ±f baÅŸÄ±na gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±nÄ± hesapla\n",
        "class_counts = {}\n",
        "for class_name in CLASS_NAMES:\n",
        "    train_count = sum(1 for label in train_labels if CLASS_NAMES[label] == class_name)\n",
        "    val_count = sum(1 for label in val_labels if CLASS_NAMES[label] == class_name)\n",
        "    test_count = sum(1 for label in test_labels if CLASS_NAMES[label] == class_name)\n",
        "    class_counts[class_name] = {'train': train_count, 'val': val_count, 'test': test_count}\n",
        "\n",
        "print(f\"\\nâœ… Dataset YÃ¼klendi:\")\n",
        "print(f\"  - Training: {len(train_dataset)} gÃ¶rÃ¼ntÃ¼\")\n",
        "print(f\"  - Validation: {len(val_dataset)} gÃ¶rÃ¼ntÃ¼\")\n",
        "print(f\"  - Test: {len(test_dataset)} gÃ¶rÃ¼ntÃ¼\")\n",
        "print(f\"  - Toplam: {len(train_dataset) + len(val_dataset) + len(test_dataset)} gÃ¶rÃ¼ntÃ¼\")\n",
        "print(f\"\\nğŸ“Š SÄ±nÄ±f BaÅŸÄ±na GÃ¶rÃ¼ntÃ¼ SayÄ±sÄ±:\")\n",
        "for class_name in CLASS_NAMES:\n",
        "    counts = class_counts[class_name]\n",
        "    print(f\"  - {class_name}: Train={counts['train']}, Val={counts['val']}, Test={counts['test']}\")\n",
        "print(f\"\\nâš™ï¸  EÄŸitim Parametreleri:\")\n",
        "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  - Epochs: {EPOCHS}\")\n",
        "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"  - Steps per Epoch: {len(train_loader)}\")\n",
        "print(f\"  - Toplam EÄŸitim AdÄ±mÄ±: {len(train_loader) * EPOCHS}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DisasterCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    5 sÄ±nÄ±flÄ± afet hasarÄ± gÃ¶rÃ¼ntÃ¼lerini sÄ±nÄ±flandÄ±rmak iÃ§in CNN modeli\n",
        "    \n",
        "    Mimari:\n",
        "    - 3 Convolutional katman\n",
        "    - Batch Normalization\n",
        "    - Max Pooling\n",
        "    - Dropout\n",
        "    - Fully Connected katmanlar\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=5):\n",
        "        super(DisasterCNN, self).__init__()\n",
        "        \n",
        "        # Ä°lk Convolutional Blok\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Ä°kinci Convolutional Blok\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # ÃœÃ§Ã¼ncÃ¼ Convolutional Blok\n",
        "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        \n",
        "        # Fully Connected Katmanlar (Dropout oranlarÄ± optimize edildi)\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.dropout1 = nn.Dropout(0.6)  # ArtÄ±rÄ±ldÄ± - training sÄ±rasÄ±nda daha fazla regularization\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.dropout2 = nn.Dropout(0.6)  # ArtÄ±rÄ±ldÄ± - training sÄ±rasÄ±nda daha fazla regularization\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Ä°lk blok\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        # Ä°kinci blok\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        # ÃœÃ§Ã¼ncÃ¼ blok\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool3(x)\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        x = self.avgpool(x)\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Fully Connected\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def get_model(num_classes=5, device='cuda'):\n",
        "    \"\"\"\n",
        "    Model oluÅŸturur ve belirtilen cihaza yÃ¼kler\n",
        "    \n",
        "    Args:\n",
        "        num_classes: SÄ±nÄ±f sayÄ±sÄ± (5: afet hasarÄ± kategorileri)\n",
        "        device: 'cuda' veya 'cpu'\n",
        "    \n",
        "    Returns:\n",
        "        Model\n",
        "    \"\"\"\n",
        "    model = DisasterCNN(num_classes=num_classes)\n",
        "    model = model.to(device)\n",
        "    return model\n",
        "\n",
        "# Model Ã¶zeti dosyasÄ±na yaz\n",
        "def get_model_summary(model, num_classes, class_names, img_size, batch_size, learning_rate, epochs):\n",
        "    \"\"\"Model Ã¶zetini string olarak dÃ¶ndÃ¼rÃ¼r\"\"\"\n",
        "    summary = []\n",
        "    summary.append(\"=\"*60)\n",
        "    summary.append(\"MODEL Ã–ZETÄ°\")\n",
        "    summary.append(\"=\"*60)\n",
        "    summary.append(\"\")\n",
        "    summary.append(f\"Model: DisasterCNN\")\n",
        "    summary.append(f\"SÄ±nÄ±f SayÄ±sÄ±: {num_classes}\")\n",
        "    summary.append(f\"SÄ±nÄ±flar: {class_names}\")\n",
        "    summary.append(\"\")\n",
        "    summary.append(\"=\"*60)\n",
        "    summary.append(\"MÄ°MARÄ°\")\n",
        "    summary.append(\"=\"*60)\n",
        "    summary.append(\"\")\n",
        "    summary.append(\"Conv Block 1: Conv2d(3,64) + BatchNorm + ReLU + MaxPool2d\")\n",
        "    summary.append(\"Conv Block 2: Conv2d(64,128) + BatchNorm + ReLU + MaxPool2d\")\n",
        "    summary.append(\"Conv Block 3: Conv2d(128,256) + BatchNorm + ReLU + MaxPool2d\")\n",
        "    summary.append(\"Global Average Pooling: AdaptiveAvgPool2d((1,1))\")\n",
        "    summary.append(\"FC Layers: Linear(256,128) + ReLU + Dropout(0.6)\")\n",
        "    summary.append(\"           Linear(128,64) + ReLU + Dropout(0.6)\")\n",
        "    summary.append(\"           Linear(64,{})\".format(num_classes))\n",
        "    summary.append(\"\")\n",
        "    summary.append(\"=\"*60)\n",
        "    summary.append(\"PARAMETRELER\")\n",
        "    summary.append(\"=\"*60)\n",
        "    summary.append(\"\")\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    summary.append(f\"Toplam Parametre: {total:,}\")\n",
        "    summary.append(f\"EÄŸitilebilir Parametre: {trainable:,}\")\n",
        "    summary.append(\"\")\n",
        "    summary.append(\"=\"*60)\n",
        "    summary.append(\"HÄ°PERPARAMETRELER\")\n",
        "    summary.append(\"=\"*60)\n",
        "    summary.append(\"\")\n",
        "    summary.append(f\"GÃ¶rÃ¼ntÃ¼ Boyutu: {img_size}x{img_size}\")\n",
        "    summary.append(f\"Batch Size: {batch_size}\")\n",
        "    summary.append(f\"Learning Rate: {learning_rate}\")\n",
        "    summary.append(f\"Epochs: {epochs}\")\n",
        "    summary.append(f\"Optimizer: Adam (weight_decay=1e-4)\")\n",
        "    summary.append(f\"Loss: CrossEntropyLoss\")\n",
        "    summary.append(f\"Scheduler: ReduceLROnPlateau (patience=7)\")\n",
        "    summary.append(f\"Dropout: 0.6\")\n",
        "    summary.append(f\"Data Augmentation: GÃ¼Ã§lendirilmiÅŸ (ColorJitter, artÄ±rÄ±lmÄ±ÅŸ rotation/affine)\")\n",
        "    summary.append(\"\")\n",
        "    return \"\\n\".join(summary)\n",
        "\n",
        "print(\"âœ… Model tanÄ±mÄ± hazÄ±r!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. EÄŸitim ve DoÄŸrulama FonksiyonlarÄ±\n",
        "\n",
        "Model eÄŸitimi ve doÄŸrulama iÃ§in ana fonksiyonlar tanÄ±mlanÄ±yor.\n",
        "\n",
        "**Fonksiyonlar:**\n",
        "- `train_epoch()`: Bir epoch boyunca model eÄŸitimi\n",
        "  - Forward ve backward pass\n",
        "  - Loss ve accuracy hesaplama\n",
        "  - Progress bar ile ilerleme gÃ¶sterimi\n",
        "  \n",
        "- `validate()`: Model doÄŸrulama\n",
        "  - Test/validation seti Ã¼zerinde deÄŸerlendirme\n",
        "  - Loss ve accuracy hesaplama\n",
        "  - Tahminleri dÃ¶ndÃ¼rme\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    \"\"\"CUDA kullanÄ±labilir mi kontrol et\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"CUDA kullanÄ±lÄ±yor: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"CPU kullanÄ±lÄ±yor\")\n",
        "    return device\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, loss, accuracy, filepath):\n",
        "    \"\"\"Model checkpoint kaydet\"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "        'accuracy': accuracy\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint kaydedildi: {filepath}\")\n",
        "\n",
        "\n",
        "def plot_training_history(history, save_path='training_history.png'):\n",
        "    \"\"\"EÄŸitim geÃ§miÅŸini gÃ¶rselleÅŸtir - 2 grafik (Ã¼st Ã¼ste) - fotoÄŸraftaki gibi\"\"\"\n",
        "    os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
        "    \n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    \n",
        "    # 1. Model Loss (Train & Validation) - Ãœst\n",
        "    axes[0].plot(epochs, history['train_loss'], label='Train Loss', \n",
        "                color='#1f77b4', marker='s', linewidth=2.5, markersize=4, alpha=0.9)\n",
        "    axes[0].plot(epochs, history['val_loss'], label='Validation Loss', \n",
        "                color='#d62728', marker='s', linewidth=2.5, markersize=4, alpha=0.9)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_title('Model Loss (Train & Validation)', fontsize=14, fontweight='bold', pad=10)\n",
        "    axes[0].legend(fontsize=11, loc='best', framealpha=0.9)\n",
        "    axes[0].grid(True, alpha=0.3, linestyle='--', linewidth=0.8)\n",
        "    axes[0].set_xticks(range(0, len(epochs) + 1, max(1, len(epochs) // 10)))\n",
        "    axes[0].set_xlim(0, len(epochs) + 1)\n",
        "    \n",
        "    # 2. Model Accuracy (Train & Validation) - Alt\n",
        "    axes[1].plot(epochs, history['train_acc'], label='Train Accuracy', \n",
        "                color='#2ca02c', marker='s', linewidth=2.5, markersize=4, alpha=0.9)\n",
        "    axes[1].plot(epochs, history['val_acc'], label='Validation Accuracy', \n",
        "                color='#ff7f0e', marker='s', linewidth=2.5, markersize=4, alpha=0.9)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_title('Model Accuracy (Train & Validation)', fontsize=14, fontweight='bold', pad=10)\n",
        "    axes[1].legend(fontsize=11, loc='best', framealpha=0.9)\n",
        "    axes[1].grid(True, alpha=0.3, linestyle='--', linewidth=0.8)\n",
        "    axes[1].set_xticks(range(0, len(epochs) + 1, max(1, len(epochs) // 10)))\n",
        "    axes[1].set_xlim(0, len(epochs) + 1)\n",
        "    \n",
        "    plt.tight_layout(pad=3.0)\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Grafik kaydedildi: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names, save_path='confusion_matrix.png'):\n",
        "    \"\"\"Confusion matrix gÃ¶rselleÅŸtir\"\"\"\n",
        "    os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.ylabel('GerÃ§ek Etiket')\n",
        "    plt.xlabel('Tahmin Edilen Etiket')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Confusion matrix kaydedildi: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def print_classification_report(y_true, y_pred, class_names):\n",
        "    \"\"\"SÄ±nÄ±flandÄ±rma raporu yazdÄ±r\"\"\"\n",
        "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SINIFLANDIRMA RAPORU\")\n",
        "    print(\"=\"*50)\n",
        "    print(report)\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "def plot_predictions(images, labels, predictions, class_names, save_path, title, max_samples=16):\n",
        "    \"\"\"\n",
        "    DoÄŸru veya yanlÄ±ÅŸ tahminleri gÃ¶rselleÅŸtir\n",
        "    \n",
        "    Args:\n",
        "        images: GÃ¶rÃ¼ntÃ¼ yollarÄ± listesi\n",
        "        labels: GerÃ§ek etiketler\n",
        "        predictions: Tahmin edilen etiketler\n",
        "        class_names: SÄ±nÄ±f isimleri\n",
        "        save_path: KayÄ±t yolu\n",
        "        title: Grafik baÅŸlÄ±ÄŸÄ±\n",
        "        max_samples: Maksimum gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±\n",
        "    \"\"\"\n",
        "    os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n",
        "    \n",
        "    # GÃ¶rÃ¼ntÃ¼leri sÄ±nÄ±rla\n",
        "    num_samples = min(len(images), max_samples)\n",
        "    \n",
        "    # Grid boyutunu hesapla (4 sÃ¼tun)\n",
        "    rows = (num_samples + 3) // 4\n",
        "    cols = 4\n",
        "    \n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(16, 4 * rows))\n",
        "    if rows == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx in range(num_samples):\n",
        "        ax = axes[idx]\n",
        "        \n",
        "        try:\n",
        "            # GÃ¶rÃ¼ntÃ¼yÃ¼ yÃ¼kle\n",
        "            image = Image.open(images[idx]).convert('RGB')\n",
        "            ax.imshow(image)\n",
        "            \n",
        "            # Etiket bilgileri\n",
        "            true_label = class_names[labels[idx]]\n",
        "            pred_label = class_names[predictions[idx]]\n",
        "            is_correct = labels[idx] == predictions[idx]\n",
        "            \n",
        "            # BaÅŸlÄ±k rengi (doÄŸru: yeÅŸil, yanlÄ±ÅŸ: kÄ±rmÄ±zÄ±)\n",
        "            color = '#2ca02c' if is_correct else '#d62728'\n",
        "            title_text = f\"GerÃ§ek: {true_label}\\nTahmin: {pred_label}\"\n",
        "            \n",
        "            ax.set_title(title_text, fontsize=10, fontweight='bold', color=color, pad=5)\n",
        "            ax.axis('off')\n",
        "            \n",
        "        except Exception as e:\n",
        "            ax.text(0.5, 0.5, f'Hata:\\n{str(e)[:30]}', \n",
        "                   ha='center', va='center', fontsize=8, color='red')\n",
        "            ax.axis('off')\n",
        "    \n",
        "    # KullanÄ±lmayan subplot'larÄ± gizle\n",
        "    for idx in range(num_samples, len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.suptitle(title, fontsize=16, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"GÃ¶rselleÅŸtirme kaydedildi: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def visualize_correct_and_incorrect_predictions(test_images, test_labels, test_preds, class_names):\n",
        "    \"\"\"\n",
        "    DoÄŸru ve yanlÄ±ÅŸ tahminleri ayrÄ± ayrÄ± gÃ¶rselleÅŸtir\n",
        "    \n",
        "    Args:\n",
        "        test_images: Test gÃ¶rÃ¼ntÃ¼ yollarÄ±\n",
        "        test_labels: GerÃ§ek etiketler\n",
        "        test_preds: Tahmin edilen etiketler\n",
        "        class_names: SÄ±nÄ±f isimleri\n",
        "    \"\"\"\n",
        "    # DoÄŸru ve yanlÄ±ÅŸ tahminleri ayÄ±r\n",
        "    correct_indices = []\n",
        "    incorrect_indices = []\n",
        "    \n",
        "    for idx in range(len(test_labels)):\n",
        "        if test_labels[idx] == test_preds[idx]:\n",
        "            correct_indices.append(idx)\n",
        "        else:\n",
        "            incorrect_indices.append(idx)\n",
        "    \n",
        "    print(f\"\\nğŸ“Š Tahmin Ä°statistikleri:\")\n",
        "    print(f\"  - DoÄŸru Tahmin: {len(correct_indices)}/{len(test_labels)} ({100*len(correct_indices)/len(test_labels):.2f}%)\")\n",
        "    print(f\"  - YanlÄ±ÅŸ Tahmin: {len(incorrect_indices)}/{len(test_labels)} ({100*len(incorrect_indices)/len(test_labels):.2f}%)\")\n",
        "    \n",
        "    # DoÄŸru tahminleri gÃ¶rselleÅŸtir\n",
        "    if len(correct_indices) > 0:\n",
        "        correct_images = [test_images[idx] for idx in correct_indices]\n",
        "        correct_labels = [test_labels[idx] for idx in correct_indices]\n",
        "        correct_preds = [test_preds[idx] for idx in correct_indices]\n",
        "        \n",
        "        plot_predictions(\n",
        "            correct_images, correct_labels, correct_preds, class_names,\n",
        "            'correct_predictions.png',\n",
        "            f'DoÄŸru Tahminler ({len(correct_indices)}/{len(test_labels)})',\n",
        "            max_samples=16\n",
        "        )\n",
        "    else:\n",
        "        print(\"âš ï¸ DoÄŸru tahmin bulunamadÄ±!\")\n",
        "    \n",
        "    # YanlÄ±ÅŸ tahminleri gÃ¶rselleÅŸtir\n",
        "    if len(incorrect_indices) > 0:\n",
        "        incorrect_images = [test_images[idx] for idx in incorrect_indices]\n",
        "        incorrect_labels = [test_labels[idx] for idx in incorrect_indices]\n",
        "        incorrect_preds = [test_preds[idx] for idx in incorrect_indices]\n",
        "        \n",
        "        plot_predictions(\n",
        "            incorrect_images, incorrect_labels, incorrect_preds, class_names,\n",
        "            'incorrect_predictions.png',\n",
        "            f'YanlÄ±ÅŸ Tahminler ({len(incorrect_indices)}/{len(test_labels)})',\n",
        "            max_samples=16\n",
        "        )\n",
        "    else:\n",
        "        print(\"âš ï¸ YanlÄ±ÅŸ tahmin bulunamadÄ±!\")\n",
        "\n",
        "print(\"âœ… YardÄ±mcÄ± fonksiyonlar hazÄ±r!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training ve validation fonksiyonlarÄ±\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Bir epoch eÄŸitim\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc='EÄŸitim')\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Ä°statistikler\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Progress bar gÃ¼ncelle\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'acc': f'{100*correct/total:.2f}%'\n",
        "        })\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"DoÄŸrulama\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(val_loader, desc='DoÄŸrulama')\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100*correct/total:.2f}%'\n",
        "            })\n",
        "    \n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
        "\n",
        "print(\"âœ… EÄŸitim fonksiyonlarÄ± hazÄ±r!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model EÄŸitimi\n",
        "\n",
        "CNN modelini eÄŸitiyoruz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model oluÅŸtur\n",
        "model = get_model(NUM_CLASSES, device)\n",
        "\n",
        "# Model Ã¶zeti\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Toplam parametre sayÄ±sÄ±: {total_params:,}\")\n",
        "\n",
        "# Model Ã¶zetini dosyaya yaz\n",
        "model_summary_text = get_model_summary(model, NUM_CLASSES, CLASS_NAMES, IMG_SIZE, BATCH_SIZE, LEARNING_RATE, EPOCHS)\n",
        "with open('model_summary.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(model_summary_text)\n",
        "print(\"âœ… Model Ã¶zeti kaydedildi: model_summary.txt\")\n",
        "\n",
        "# Loss ve Optimizer (Optimize edildi - daha dengeli eÄŸitim iÃ§in)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)  # Weight decay eklendi\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=7, min_lr=1e-6  # Patience artÄ±rÄ±ldÄ±\n",
        ")\n",
        "\n",
        "# EÄŸitim geÃ§miÅŸini tutacak sÃ¶zlÃ¼k\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_val_loss = float('inf')\n",
        "best_model_state = None\n",
        "patience_counter = 0\n",
        "patience = 10  # Early stopping patience\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸš€ EÄÄ°TÄ°M BAÅLIYOR\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nğŸ“Š Dataset Bilgileri:\")\n",
        "print(f\"  - EÄŸitim GÃ¶rÃ¼ntÃ¼sÃ¼: {len(train_dataset)}\")\n",
        "print(f\"  - DoÄŸrulama GÃ¶rÃ¼ntÃ¼sÃ¼: {len(val_dataset)}\")\n",
        "print(f\"  - Test GÃ¶rÃ¼ntÃ¼sÃ¼: {len(test_dataset)}\")\n",
        "print(f\"  - Toplam GÃ¶rÃ¼ntÃ¼: {len(train_dataset) + len(val_dataset) + len(test_dataset)}\")\n",
        "print(f\"\\nâš™ï¸  EÄŸitim Parametreleri:\")\n",
        "print(f\"  - Epoch SayÄ±sÄ±: {EPOCHS}\")\n",
        "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"  - Steps per Epoch: {len(train_loader)}\")\n",
        "print(f\"  - Toplam EÄŸitim AdÄ±mÄ±: {len(train_loader) * EPOCHS}\")\n",
        "print(f\"\\nğŸ¯ Hedef: YÃ¼ksek Accuracy, DÃ¼ÅŸÃ¼k Loss\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# EÄŸitim dÃ¶ngÃ¼sÃ¼\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    # EÄŸitim\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model, train_loader, criterion, optimizer, device\n",
        "    )\n",
        "    \n",
        "    # DoÄŸrulama\n",
        "    val_loss, val_acc, val_preds, val_labels = validate(\n",
        "        model, val_loader, criterion, device\n",
        "    )\n",
        "    \n",
        "    # Learning rate scheduler\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # GeÃ§miÅŸe ekle\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    \n",
        "    # SonuÃ§larÄ± yazdÄ±r (validation loss'un train loss'tan dÃ¼ÅŸÃ¼k olup olmadÄ±ÄŸÄ±nÄ± gÃ¶ster)\n",
        "    loss_diff = train_loss - val_loss\n",
        "    acc_diff = val_acc - train_acc\n",
        "    loss_status = \"âœ“\" if val_loss < train_loss else \"âš \"\n",
        "    acc_status = \"âœ“\" if val_acc > train_acc else \"âš \"\n",
        "    \n",
        "    print(f\"\\nEpoch {epoch} SonuÃ§larÄ±:\")\n",
        "    print(f\"  EÄŸitim Loss: {train_loss:.4f} | EÄŸitim Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"  DoÄŸrulama Loss: {val_loss:.4f} | DoÄŸrulama Accuracy: {val_acc:.2f}%\")\n",
        "    print(f\"  {loss_status} Loss FarkÄ±: {loss_diff:+.4f} (Val {'dÃ¼ÅŸÃ¼k' if val_loss < train_loss else 'yÃ¼ksek'})\")\n",
        "    print(f\"  {acc_status} Accuracy FarkÄ±: {acc_diff:+.2f}% (Val {'yÃ¼ksek' if val_acc > train_acc else 'dÃ¼ÅŸÃ¼k'})\")\n",
        "    \n",
        "    # En iyi modeli kaydet\n",
        "    if val_acc > best_val_acc or (val_acc == best_val_acc and val_loss < best_val_loss):\n",
        "        best_val_acc = val_acc\n",
        "        best_val_loss = val_loss\n",
        "        best_model_state = model.state_dict().copy()\n",
        "        checkpoint_path = 'best_model.pth'\n",
        "        save_checkpoint(\n",
        "            model, optimizer, epoch, val_loss, val_acc, checkpoint_path\n",
        "        )\n",
        "        print(f\"  âœ“ Yeni en iyi model kaydedildi! (Accuracy: {val_acc:.2f}%)\")\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    \n",
        "    # Early Stopping\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"\\nâš ï¸ Early Stopping: {patience} epoch boyunca iyileÅŸme olmadÄ±\")\n",
        "        print(f\"   En iyi Val Loss: {best_val_loss:.4f}, En iyi Val Acc: {best_val_acc:.2f}%\")\n",
        "        break\n",
        "\n",
        "# En iyi modeli yÃ¼kle\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EÄÄ°TÄ°M TAMAMLANDI!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Toplam sÃ¼re: {total_time/60:.2f} dakika\")\n",
        "print(f\"En iyi doÄŸrulama accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"En iyi doÄŸrulama loss: {best_val_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. SonuÃ§larÄ± GÃ¶rselleÅŸtirme\n",
        "\n",
        "EÄŸitim geÃ§miÅŸini gÃ¶rselleÅŸtiriyoruz (2 grafik - Ã¼st Ã¼ste).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training history grafiÄŸi (4 ayrÄ± grafik - 2x2 grid)\n",
        "plot_training_history(history, 'training_history.png')\n",
        "print(\"âœ… EÄŸitim geÃ§miÅŸi grafiÄŸi kaydedildi: training_history.png\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Testi\n",
        "\n",
        "EÄŸitilmiÅŸ modeli test ediyoruz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final doÄŸrulama iÃ§in confusion matrix\n",
        "print(\"\\nFinal doÄŸrulama yapÄ±lÄ±yor...\")\n",
        "final_val_loss, final_val_acc, final_preds, final_labels = validate(\n",
        "    model, val_loader, criterion, device\n",
        ")\n",
        "\n",
        "# Test seti Ã¼zerinde deÄŸerlendirme\n",
        "print(\"\\nTest seti Ã¼zerinde deÄŸerlendirme yapÄ±lÄ±yor...\")\n",
        "test_loss, test_acc, test_preds, test_labels = validate(\n",
        "    model, test_loader, criterion, device\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Test SonuÃ§larÄ±\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Test DoÄŸruluÄŸu: {test_acc:.2f}%\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Classification Report\n",
        "report = classification_report(test_labels, test_preds, target_names=CLASS_NAMES)\n",
        "print(f\"\\nClassification Report:\\n{report}\")\n",
        "\n",
        "# Classification report'u dosyaya kaydet\n",
        "with open('classification_report.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(\"Classification Report\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\")\n",
        "    f.write(report)\n",
        "print(\"âœ… Classification report kaydedildi: classification_report.txt\")\n",
        "\n",
        "# Classification report tablosu gÃ¶rselleÅŸtirme\n",
        "report_dict = classification_report(test_labels, test_preds, target_names=CLASS_NAMES, output_dict=True)\n",
        "df_report = pd.DataFrame(report_dict).transpose()\n",
        "df_report = df_report.iloc[:-3]  # Son 3 satÄ±rÄ± (accuracy, macro avg, weighted avg) al\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df_report[['precision', 'recall', 'f1-score']], annot=True, fmt='.3f', cmap='Blues', cbar_kws={'label': 'DeÄŸer'})\n",
        "plt.title('Classification Report - Precision, Recall, F1-Score', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('SÄ±nÄ±f')\n",
        "plt.xlabel('Metrik')\n",
        "plt.tight_layout()\n",
        "plt.savefig('classification_report_table.png', dpi=300, bbox_inches='tight')\n",
        "print(\"âœ… Classification report tablosu kaydedildi: classification_report_table.png\")\n",
        "plt.close()\n",
        "\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix(test_labels, test_preds, CLASS_NAMES, 'confusion_matrix.png')\n",
        "\n",
        "# SÄ±nÄ±flandÄ±rma raporu\n",
        "print_classification_report(test_labels, test_preds, CLASS_NAMES)\n",
        "\n",
        "# DoÄŸru ve yanlÄ±ÅŸ tahminleri gÃ¶rselleÅŸtir\n",
        "visualize_correct_and_incorrect_predictions(test_images, test_labels, test_preds, CLASS_NAMES)\n",
        "\n",
        "# Model kaydet\n",
        "torch.save(model.state_dict(), 'best_model.pth')\n",
        "print(\"âœ… Model kaydedildi: best_model.pth\")\n",
        "\n",
        "print(\"\\nTÃ¼m sonuÃ§lar kaydedildi!\")\n",
        "print(f\"  - Model: best_model.pth\")\n",
        "print(f\"  - Model Ã–zeti: model_summary.txt\")\n",
        "print(f\"  - Grafik: training_history.png\")\n",
        "print(f\"  - Confusion Matrix: confusion_matrix.png\")\n",
        "print(f\"  - Classification Report: classification_report.txt, classification_report_table.png\")\n",
        "print(f\"  - DoÄŸru Tahminler: correct_predictions.png\")\n",
        "print(f\"  - YanlÄ±ÅŸ Tahminler: incorrect_predictions.png\")\n",
        "print(f\"{'='*60}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Ã–rnek Tahminler\n",
        "\n",
        "EÄŸitilmiÅŸ model ile sÄ±nÄ±flardan Ã¶rnek gÃ¶rÃ¼ntÃ¼ler Ã¼zerinde tahmin yapÄ±yoruz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_image(model, image_path, class_names, device, transform=None):\n",
        "    \"\"\"\n",
        "    Tek bir gÃ¶rÃ¼ntÃ¼ iÃ§in tahmin yap ve gÃ¶rselleÅŸtir\n",
        "    \n",
        "    Args:\n",
        "        model: EÄŸitilmiÅŸ model\n",
        "        image_path: Test gÃ¶rÃ¼ntÃ¼sÃ¼ yolu\n",
        "        class_names: SÄ±nÄ±f isimleri\n",
        "        device: Cihaz (cuda/cpu)\n",
        "        transform: GÃ¶rÃ¼ntÃ¼ transformu\n",
        "    \n",
        "    Returns:\n",
        "        Tahmin edilen sÄ±nÄ±f, gÃ¼ven skoru ve olasÄ±lÄ±klar\n",
        "    \"\"\"\n",
        "    # Modeli eval moduna al\n",
        "    model.eval()\n",
        "    \n",
        "    # Transform yoksa validation transform kullan\n",
        "    if transform is None:\n",
        "        transform = val_transform\n",
        "    \n",
        "    # GÃ¶rÃ¼ntÃ¼yÃ¼ yÃ¼kle ve dÃ¶nÃ¼ÅŸtÃ¼r\n",
        "    try:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "    except Exception as e:\n",
        "        print(f\"Hata: {image_path} yÃ¼klenemedi: {e}\")\n",
        "        return None, None, None\n",
        "    \n",
        "    # Tahmin yap\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "        probabilities = F.softmax(output, dim=1)\n",
        "        confidence, predicted = torch.max(probabilities, 1)\n",
        "    \n",
        "    predicted_class = class_names[predicted.item()]\n",
        "    confidence_score = confidence.item() * 100\n",
        "    \n",
        "    # TÃ¼m sÄ±nÄ±f olasÄ±lÄ±klarÄ±nÄ± al\n",
        "    all_probs = {}\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        all_probs[class_name] = probabilities[0][i].item() * 100\n",
        "    \n",
        "    return predicted_class, confidence_score, all_probs\n",
        "\n",
        "\n",
        "# Her sÄ±nÄ±ftan Ã¶rnek gÃ¶rÃ¼ntÃ¼ seÃ§ ve tahmin yap\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Ã–RNEK TAHMÄ°NLER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test setinden her sÄ±nÄ±ftan bir Ã¶rnek seÃ§\n",
        "examples = {}\n",
        "\n",
        "for class_name in CLASS_NAMES:\n",
        "    # Test setinde bu sÄ±nÄ±ftan gÃ¶rÃ¼ntÃ¼ ara\n",
        "    class_images = [img for img, label in zip(test_images, test_labels) \n",
        "                    if CLASS_NAMES[label] == class_name]\n",
        "    \n",
        "    if len(class_images) > 0:\n",
        "        # Rastgele bir gÃ¶rÃ¼ntÃ¼ seÃ§\n",
        "        example_image = random.choice(class_images)\n",
        "        examples[class_name] = example_image\n",
        "\n",
        "# Her sÄ±nÄ±f iÃ§in tahmin yap\n",
        "for class_name, image_path in examples.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"SÄ±nÄ±f: {class_name.upper()}\")\n",
        "    print(f\"GÃ¶rÃ¼ntÃ¼: {os.path.basename(image_path)}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    predicted_class, confidence, all_probs = predict_image(\n",
        "        model, image_path, CLASS_NAMES, device, val_transform\n",
        "    )\n",
        "    \n",
        "    if predicted_class:\n",
        "        print(f\"\\nâœ… Tahmin: {predicted_class.upper()}\")\n",
        "        print(f\"ğŸ“Š GÃ¼ven: {confidence:.2f}%\")\n",
        "        print(f\"\\nğŸ“ˆ TÃ¼m SÄ±nÄ±f OlasÄ±lÄ±klarÄ±:\")\n",
        "        for cls_name, prob in sorted(all_probs.items(), key=lambda x: x[1], reverse=True):\n",
        "            marker = \" â†\" if cls_name == predicted_class else \"\"\n",
        "            print(f\"  - {cls_name}: {prob:.2f}%{marker}\")\n",
        "        \n",
        "        # GÃ¶rselleÅŸtirme\n",
        "        try:\n",
        "            original_image = Image.open(image_path).convert('RGB')\n",
        "            \n",
        "            fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "            \n",
        "            # Sol: GÃ¶rÃ¼ntÃ¼\n",
        "            axes[0].imshow(original_image)\n",
        "            axes[0].axis('off')\n",
        "            axes[0].set_title(f'GÃ¶rÃ¼ntÃ¼: {os.path.basename(image_path)}\\nGerÃ§ek: {class_name.upper()}\\nTahmin: {predicted_class.upper()} ({confidence:.2f}%)', \n",
        "                            fontsize=12, fontweight='bold', pad=10)\n",
        "            \n",
        "            # SaÄŸ: OlasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±\n",
        "            classes = list(all_probs.keys())\n",
        "            probs = [all_probs[cls] for cls in classes]\n",
        "            colors = ['#ff6b6b' if cls == predicted_class else '#95a5a6' for cls in classes]\n",
        "            bars = axes[1].barh(classes, probs, color=colors)\n",
        "            axes[1].set_xlabel('OlasÄ±lÄ±k (%)', fontsize=11, fontweight='bold')\n",
        "            axes[1].set_title('SÄ±nÄ±f OlasÄ±lÄ±klarÄ±', fontsize=12, fontweight='bold')\n",
        "            axes[1].set_xlim(0, 100)\n",
        "            axes[1].grid(axis='x', alpha=0.3)\n",
        "            \n",
        "            # Bar Ã¼zerine deÄŸerleri yaz\n",
        "            for i, (bar, prob) in enumerate(zip(bars, probs)):\n",
        "                width = bar.get_width()\n",
        "                axes[1].text(width + 1, bar.get_y() + bar.get_height()/2, \n",
        "                            f'{prob:.2f}%', \n",
        "                            ha='left', va='center', fontsize=10, \n",
        "                            fontweight='bold' if classes[i] == predicted_class else 'normal')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ GÃ¶rselleÅŸtirme hatasÄ±: {e}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"âœ… Ã–rnek tahminler tamamlandÄ±!\")\n",
        "print(f\"{'='*60}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sinirproje",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
